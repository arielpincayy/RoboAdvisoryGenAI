{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9987643",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=\"api\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb60d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Usando dispositivo: cuda\n",
      "üîÑ Reinicializando recursos NLTK despu√©s de deserializaci√≥n...\n",
      "üì• Descargando recurso NLTK: wordnet\n",
      "üì• Descargando recurso NLTK: omw-1.4\n",
      "‚úÖ TextPreprocessor deserializado correctamente\n",
      "‚úÖ Modelo cargado: GLOVE.pt\n",
      "‚úÖ Modelo cargado: SBERT.pt\n",
      "‚úÖ Modelo cargado: Word2Vec.pt\n",
      "üîÑ Corrigiendo dimensi√≥n oculta: 8 -> 8\n",
      "\n",
      "‚ùå Error: Error(s) in loading state_dict for RecommendationModel:\n",
      "\tsize mismatch for layer1.weight: copying a param with shape torch.Size([8, 96]) from checkpoint, the shape in current model is torch.Size([8, 98]).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from openai import OpenAI\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from gensim.models import Word2Vec\n",
    "import joblib\n",
    "\n",
    "# ===== CONFIG =====\n",
    "API_KEY = api_key\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"‚ö†Ô∏è No se encontr√≥ la variable de entorno OPENAI_API_KEY\")\n",
    "\n",
    "PATH_PIPELINE = \"pipelines\"\n",
    "PATH_MODELS = \"models\"\n",
    "PATH_DATA = \"data\"\n",
    "\n",
    "lista_tarjetas = [\n",
    "    \"Joy\", \"Oro\", \"Clasica\", \"Platinum\", \"Descubre\", \"Explora\", \n",
    "    \"Conquista\", \"Line Up\", \"La Comer\", \"Costo\", \"Home Depot\", \n",
    "    \"Affinity\", \"Teleton\"\n",
    "]\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. ARQUITECTURAS (FINAL)\n",
    "# ==============================================================================\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes=3):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 8) \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(8, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "class RecommendationModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, output_dim=13):\n",
    "        super(RecommendationModel, self).__init__()\n",
    "        # Ajustado a las llaves 'layer1' y 'layer2' del log de error\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        return self.layer2(x)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PREPROCESAMIENTO (Igual que antes)\n",
    "# ==============================================================================\n",
    "# ... (Mant√©n las clases TextPreprocessor, DateFeatureGenerator, DropColumns, DynamicPreprocessor igual que antes)\n",
    "# Para ahorrar espacio, asumo que ya tienes estas clases definidas o importadas.\n",
    "# Si necesitas que las repita completas, av√≠same.\n",
    "# Aqu√≠ pego una versi√≥n resumida para que el script corra si lo copias entero:\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, use_bigrams=True, use_trigrams=True, glove_path=None,\n",
    "                 sbert_model_name='all-MiniLM-L6-v2', w2v_model_path=None):\n",
    "        self.use_bigrams = use_bigrams\n",
    "        self.use_trigrams = use_trigrams\n",
    "        self.glove_path = glove_path\n",
    "        self.glove = {}\n",
    "        self.sbert_model_name = sbert_model_name\n",
    "        self.sbert_model = None\n",
    "        self.w2v_model = None\n",
    "        self.w2v_model_path = w2v_model_path\n",
    "        self._ensure_nltk_resources()\n",
    "        self._init_nltk_components()\n",
    "    def _ensure_nltk_resources(self):\n",
    "        import nltk; nltk.download('punkt', quiet=True); nltk.download('stopwords', quiet=True); nltk.download('wordnet', quiet=True); nltk.download('omw-1.4', quiet=True)\n",
    "    def _init_nltk_components(self):\n",
    "        from nltk.corpus import stopwords; from nltk.stem import WordNetLemmatizer\n",
    "        self.stop_words = set(stopwords.words('english')); self.lemmatizer = WordNetLemmatizer()\n",
    "    def _clean_text(self, text):\n",
    "        import re; text = str(text).lower(); text = re.sub(r\"http\\S+\", \"\", text); text = re.sub(r\"[^a-z√°√©√≠√≥√∫√º√± ]\", \"\", text); return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    def _get_wordnet_pos(self, tag):\n",
    "        from nltk.corpus import wordnet; return wordnet.VERB if tag.startswith('V') else wordnet.NOUN\n",
    "    def _tokenize_series(self, series):\n",
    "        from nltk import pos_tag; from nltk.tokenize import word_tokenize\n",
    "        all_tokens = []\n",
    "        for text in series:\n",
    "            tokens = word_tokenize(self._clean_text(text))\n",
    "            tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]\n",
    "            all_tokens.append([self.lemmatizer.lemmatize(t) for t in tokens]) # Simplificado para brevedad\n",
    "        return all_tokens\n",
    "    def _avg_vector(self, tokens, model):\n",
    "        vecs = [model.wv[w] for w in tokens if w in model.wv]\n",
    "        return np.mean(vecs, axis=0) if vecs else np.zeros(model.vector_size)\n",
    "    def fit(self, X, y=None):\n",
    "        self.X_tokens_ = self._tokenize_series(X)\n",
    "        self.w2v_model = Word2Vec(sentences=self.X_tokens_, vector_size=100, window=5, min_count=2, workers=1)\n",
    "        self.sbert_model = SentenceTransformer(self.sbert_model_name)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        if self.w2v_model is None and self.w2v_model_path and os.path.exists(self.w2v_model_path): self.w2v_model = Word2Vec.load(self.w2v_model_path)\n",
    "        if self.sbert_model is None: self.sbert_model = SentenceTransformer(self.sbert_model_name)\n",
    "        tokens = self._tokenize_series(X)\n",
    "        X_w2v = np.array([self._avg_vector(t, self.w2v_model) for t in tokens])\n",
    "        X_sbert = self.sbert_model.encode(X.tolist(), batch_size=32, show_progress_bar=False)\n",
    "        return {'w2v': X_w2v, 'glove': np.zeros((len(X), 100)), 'sbert': X_sbert} # Glove dummy\n",
    "    def __getstate__(self): d = self.__dict__.copy(); d['w2v_model']=None; d['sbert_model']=None; return d\n",
    "    def __setstate__(self, s): self.__dict__.update(s); self._ensure_nltk_resources(); self._init_nltk_components()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. CREDIT ADVISOR (CORREGIDO)\n",
    "# ==============================================================================\n",
    "\n",
    "class CreditAdvisor:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"üöÄ Usando dispositivo: {self.device}\")\n",
    "\n",
    "        # Pipelines\n",
    "        self.text_pipeline = joblib.load(os.path.join(PATH_PIPELINE, \"text_pipeline.joblib\"))\n",
    "        self.data_pipeline = joblib.load(os.path.join(PATH_PIPELINE, \"pipeline_bankchurner_preprocessing.joblib\"))\n",
    "\n",
    "        # Modelos de Sentimiento (Ya funcionaban)\n",
    "        self.model_glove = SentimentClassifier(input_dim=100)\n",
    "        self.model_sbert = SentimentClassifier(input_dim=384) \n",
    "        self.model_w2v = SentimentClassifier(input_dim=100)\n",
    "        \n",
    "        # Cargar Sentimiento\n",
    "        self._load_safe(self.model_glove, os.path.join(PATH_MODELS, \"2/GLOVE.pt\"))\n",
    "        self._load_safe(self.model_sbert, os.path.join(PATH_MODELS, \"2/SBERT.pt\"))\n",
    "        self._load_safe(self.model_w2v, os.path.join(PATH_MODELS, \"2/Word2Vec.pt\"))\n",
    "\n",
    "        # --- MODELO DE RECOMENDACI√ìN (AQU√ç EST√Å LA MAGIA) ---\n",
    "        # 1. Definimos Input=98 (seg√∫n tu error 1x98)\n",
    "        # 2. Definimos Hidden=64 (valor inicial)\n",
    "        self.reco_input_dim = 96\n",
    "        self.reco_hidden_dim = 8\n",
    "        self.model_recommend = RecommendationModel(input_dim=self.reco_input_dim, hidden_dim=self.reco_hidden_dim)\n",
    "        \n",
    "        # 3. Intentamos cargar. Si falla por tama√±o, leemos el tama√±o real del error y recargamos.\n",
    "        path_reco = os.path.join(PATH_MODELS, \"1/recommend.pth\")\n",
    "        self._load_smart_reco(path_reco)\n",
    "\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "\n",
    "    def _load_safe(self, model, path):\n",
    "        model.to(self.device)\n",
    "        if os.path.exists(path):\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "                model.eval()\n",
    "                print(f\"‚úÖ Modelo cargado: {os.path.basename(path)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error en {os.path.basename(path)}: {e}\")\n",
    "\n",
    "    def _load_smart_reco(self, path):\n",
    "        \"\"\"Intenta cargar y corrige autom√°ticamente la dimensi√≥n oculta si falla\"\"\"\n",
    "        self.model_recommend.to(self.device)\n",
    "        if not os.path.exists(path):\n",
    "            print(f\"‚ö†Ô∏è No encontrado: {path}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            state_dict = torch.load(path, map_location=self.device)\n",
    "            self.model_recommend.load_state_dict(state_dict)\n",
    "            self.model_recommend.eval()\n",
    "            print(f\"‚úÖ Recomendador cargado correctamente (Input: {self.reco_input_dim}, Hidden: {self.reco_hidden_dim})\")\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e)\n",
    "            # Detectar error de tama√±o en layer1.weight\n",
    "            if \"size mismatch for layer1.weight\" in msg:\n",
    "                # El mensaje suele ser: shape [XXX, 98] vs [64, 98]\n",
    "                # Buscamos el n√∫mero correcto en el mensaje de error\n",
    "                import re\n",
    "                # Busca el patr√≥n \"torch.Size([XXX, 98])\" donde XXX es el hidden correcto\n",
    "                match = re.search(r'torch\\.Size\\(\\[(\\d+),\\s*98\\]\\)', msg)\n",
    "                if match:\n",
    "                    correct_hidden = int(match.group(1))\n",
    "                    print(f\"üîÑ Corrigiendo dimensi√≥n oculta: {self.reco_hidden_dim} -> {correct_hidden}\")\n",
    "                    \n",
    "                    # Reinicializar modelo con el tama√±o correcto\n",
    "                    self.model_recommend = RecommendationModel(input_dim=self.reco_input_dim, hidden_dim=correct_hidden)\n",
    "                    self.model_recommend.to(self.device)\n",
    "                    self.model_recommend.load_state_dict(state_dict)\n",
    "                    self.model_recommend.eval()\n",
    "                    print(\"‚úÖ Recomendador recargado con dimensi√≥n corregida.\")\n",
    "                else:\n",
    "                    print(f\"‚ùå Error de tama√±o no recuperable autom√°ticamente: {e}\")\n",
    "            else:\n",
    "                print(f\"‚ùå Error cargando recomendador: {e}\")\n",
    "\n",
    "    def analyze_client(self, raw_text: str, client_row: pd.Series):\n",
    "        # 1. Texto\n",
    "        text_features = self.text_pipeline.transform(pd.Series([raw_text]))\n",
    "        t_w2v = torch.tensor(text_features['w2v'], dtype=torch.float32).to(self.device)\n",
    "        t_glove = torch.tensor(text_features['glove'], dtype=torch.float32).to(self.device)\n",
    "        t_sbert = torch.tensor(text_features['sbert'], dtype=torch.float32).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_glove = F.softmax(self.model_glove(t_glove), dim=1).cpu().numpy()[0]\n",
    "            pred_sbert = F.softmax(self.model_sbert(t_sbert), dim=1).cpu().numpy()[0]\n",
    "            pred_w2v = F.softmax(self.model_w2v(t_w2v), dim=1).cpu().numpy()[0]\n",
    "\n",
    "        sentiment_results = {\n",
    "            \"GLOVE\": {\"neg\": float(pred_glove[0]), \"neu\": float(pred_glove[1]), \"pos\": float(pred_glove[2])},\n",
    "            \"SBERT\": {\"neg\": float(pred_sbert[0]), \"neu\": float(pred_sbert[1]), \"pos\": float(pred_sbert[2])},\n",
    "            \"Word2Vec\": {\"neg\": float(pred_w2v[0]), \"neu\": float(pred_w2v[1]), \"pos\": float(pred_w2v[2])}\n",
    "        }\n",
    "        avg_positive = np.mean([pred_glove[2], pred_sbert[2], pred_w2v[2]])\n",
    "\n",
    "        # 2. Datos Tabulares\n",
    "        df_input = client_row.to_frame().T\n",
    "        df_processed = self.data_pipeline.transform(df_input)\n",
    "        \n",
    "        # Verificar forma antes de pasar al modelo\n",
    "        if df_processed.shape[1] != self.reco_input_dim:\n",
    "            print(f\"‚ö†Ô∏è Advertencia: Pipeline gener√≥ {df_processed.shape[1]} features, modelo espera {self.reco_input_dim}\")\n",
    "            # Si faltan/sobran columnas, esto fallar√°. \n",
    "            # Asumimos que el error 1x98 era correcto y el pipeline genera 98.\n",
    "\n",
    "        t_reco_input = torch.tensor(df_processed.values, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model_recommend(t_reco_input)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "        \n",
    "        credit_score = (probs > 0.5).astype(int).tolist()\n",
    "        decision = ', '.join([card for card, credit in zip(lista_tarjetas, credit_score) if credit == 1])\n",
    "        tarjetas_sugeridas = decision if decision else \"NINGUNA\"\n",
    "\n",
    "        # 3. GPT\n",
    "        system_prompt = \"Eres un asesor experto en productos financieros.\"\n",
    "        user_prompt = f\"\"\"\n",
    "        Datos: {client_row.to_dict()}\n",
    "        Positividad: {avg_positive:.2f}\n",
    "        Comentario: \"{raw_text}\"\n",
    "        Tarjetas Pre-calificadas: {tarjetas_sugeridas}\n",
    "        \n",
    "        Salida:\n",
    "        - An√°lisis: [Justificaci√≥n]\n",
    "        - Tarjetas a Entregar: [Lista o \"Ninguna\"]\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "                temperature=0.2, max_tokens=300\n",
    "            )\n",
    "            explanation = response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            explanation = f\"Error GPT: {e}\"\n",
    "\n",
    "        return {\n",
    "            \"sentiment\": sentiment_results,\n",
    "            \"decision_model\": decision,\n",
    "            \"gpt_explanation\": explanation\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        advisor = CreditAdvisor(api_key=API_KEY)\n",
    "        \n",
    "        df = pd.read_csv(os.path.join(PATH_DATA, \"BankChurners_merged.csv\"))\n",
    "        df.drop(columns=['NPS'], inplace=True, errors='ignore')\n",
    "        df = df.dropna(subset=['Twitter'])\n",
    "\n",
    "        idx = 5\n",
    "        client_row = df.iloc[idx].drop('Twitter')\n",
    "        text_input = df.iloc[idx]['Twitter']\n",
    "\n",
    "        result = advisor.analyze_client(text_input, client_row)\n",
    "        \n",
    "        print(\"\\nüîπ RESULTADOS üîπ\")\n",
    "        print(\"Decisi√≥n:\", result[\"decision_model\"])\n",
    "        print(\"\\n--- GPT ---\\n\", result[\"gpt_explanation\"])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
