#!/usr/bin/env python3
"""
Script para crear el pipeline de preprocesamiento de datos de clientes.
DEBE ejecutarse FUERA del Docker, en tu mÃ¡quina local.
"""

import os
import sys
import pandas as pd
from sklearn.pipeline import Pipeline
import joblib

# âš ï¸ CRÃTICO: Verificar que data_transformers.py existe
if not os.path.exists('data_transformers.py'):
    print("âŒ ERROR: No se encuentra data_transformers.py en el directorio actual")
    print("   AsegÃºrate de que data_transformers.py estÃ© en el mismo directorio")
    sys.exit(1)

# Importar desde el mÃ³dulo externo
from data_transformers import DropColumns, DynamicPreprocessor, DateFeatureGenerator

# --- Directorios ---
PATH_PIPELINE = 'pipelines'
PATH_DATA = 'data'

os.makedirs(PATH_PIPELINE, exist_ok=True)
os.makedirs(PATH_DATA, exist_ok=True)

# --- Cargar datos ---
print("="*60)
print("ğŸš€ CREANDO PIPELINE DE DATOS DE CLIENTES")
print("="*60)

CSV_PATH = os.path.join(PATH_DATA, 'BankChurners_merged.csv')
if not os.path.exists(CSV_PATH):
    print(f"âŒ ERROR: No se encuentra {CSV_PATH}")
    print("   AsegÃºrate de que el archivo CSV exista en data/")
    sys.exit(1)

print(f"\nğŸ“‚ Cargando datos desde: {CSV_PATH}")
df = pd.read_csv(CSV_PATH)

print(f"   â€¢ Filas originales: {len(df)}")
print(f"   â€¢ Columnas: {len(df.columns)}")

# Eliminar filas con valores nulos
df.dropna(inplace=True)
print(f"   â€¢ Filas despuÃ©s de dropna: {len(df)}")

# --- Crear pipeline ---
print("\nğŸ”§ Creando pipeline...")
print(f"   â€¢ DropColumns importado desde: {DropColumns.__module__}")
print(f"   â€¢ DynamicPreprocessor importado desde: {DynamicPreprocessor.__module__}")
print(f"   â€¢ DateFeatureGenerator importado desde: {DateFeatureGenerator.__module__}")

data_pipeline = Pipeline([
    ('date_feature', DateFeatureGenerator()),
    ('drop_cols', DropColumns()),
    ('preprocessor', DynamicPreprocessor())
])

# --- Entrenar ---
print("\nğŸ‹ï¸  Entrenando pipeline...")
data_pipeline.fit(df)
print("âœ… Pipeline entrenado")

# Mostrar informaciÃ³n del preprocessing
preprocessor = data_pipeline.named_steps['preprocessor']
print(f"\nğŸ“Š InformaciÃ³n del preprocessing:")
print(f"   â€¢ Columnas numÃ©ricas: {len(preprocessor.num_cols)}")
print(f"   â€¢ Columnas categÃ³ricas: {len(preprocessor.cat_cols)}")
print(f"   â€¢ Features generadas: {len(preprocessor.num_cols) + len(preprocessor.cat_feature_names)}")

# --- Guardar ---
PIPELINE_OUTPUT = os.path.join(PATH_PIPELINE, 'pipeline_bankchurner_preprocessing.joblib')

print(f"\nğŸ’¾ Guardando pipeline en: {PIPELINE_OUTPUT}")
joblib.dump(data_pipeline, PIPELINE_OUTPUT)
print("âœ… Pipeline guardado")

# --- Verificar ---
print("\nğŸ” Verificando carga del pipeline...")
loaded_pipeline = joblib.load(PIPELINE_OUTPUT)

# Verificar mÃ³dulos
date_feature = loaded_pipeline.named_steps['date_feature']
drop_cols = loaded_pipeline.named_steps['drop_cols']
preprocessor = loaded_pipeline.named_steps['preprocessor']

print(f"   â€¢ DateFeatureGenerator:")
print(f"     - Clase: {date_feature.__class__.__name__}")
print(f"     - MÃ³dulo: {date_feature.__class__.__module__}")

print(f"   â€¢ DropColumns:")
print(f"     - Clase: {drop_cols.__class__.__name__}")
print(f"     - MÃ³dulo: {drop_cols.__class__.__module__}")

print(f"   â€¢ DynamicPreprocessor:")
print(f"     - Clase: {preprocessor.__class__.__name__}")
print(f"     - MÃ³dulo: {preprocessor.__class__.__module__}")

# Verificar que estÃ¡n correctamente vinculados
if (date_feature.__class__.__module__ == 'data_transformers' and
    drop_cols.__class__.__module__ == 'data_transformers' and 
    preprocessor.__class__.__module__ == 'data_transformers'):
    print("\n   âœ… Pipeline correctamente vinculado a data_transformers.py")
else:
    print("\n   âš ï¸  ADVERTENCIA: AlgÃºn transformador no estÃ¡ vinculado correctamente")

# --- Test ---
print("\nğŸ§ª Probando transformaciÃ³n...")
test_df = df.head(5)
result = loaded_pipeline.transform(test_df)

print(f"   âœ“ Input shape: {test_df.shape}")
print(f"   âœ“ Output shape: {result.shape}")
print(f"   âœ“ Output columns: {len(result.columns)}")

# --- Resumen ---
print("\n" + "="*60)
print("ğŸ‰ Â¡PIPELINE DE DATOS CREADO EXITOSAMENTE!")
print("="*60)
print(f"\nğŸ“ Archivo generado:")
print(f"   â€¢ {PIPELINE_OUTPUT}")

print("\nğŸ“¦ Para usar en Docker:")
print("   1. AsegÃºrate de copiar data_transformers.py al contenedor")
print("   2. Copia la carpeta pipelines/ completa")
print("\nğŸ’¡ Ahora puedes construir tu imagen Docker")